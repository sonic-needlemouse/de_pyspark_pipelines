{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Задание 1\n",
        "#### Сгруппировать по ключу, просуммировать значения, вывести результат"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "rdd = sc.parallelize([(1,2), (3,4), (3,6), (4,5), (3, 4), (1, 5), (4, 1)])\n",
        "\n",
        "rdd \\\n",
        "    .groupByKey() \\\n",
        "    .map(lambda x:(x[0], sum(x[1]))) \\\n",
        "    .sortByKey(ascending=True) \\\n",
        "    .collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Задание 2\n",
        "#### Посчитать частоту встречаемости слов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "lines = sc.parallelize([\n",
        "    \"a ab abc\",\n",
        "    \"a ac abc\",\n",
        "    \"b b ab abc\"\n",
        "    ])\n",
        "\n",
        "counts = lines \\\n",
        "            .flatMap(lambda x: x.split(' ')) \\\n",
        "            .map(lambda x: (x,1)) \\\n",
        "            .reduceByKey(lambda x, y: x+y)\n",
        "\n",
        "output = counts.collect()\n",
        "\n",
        "for (word, count) in output:\n",
        "    print(\"%s: %i\" % (word, count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### market.events"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Задание 3\n",
        "#### Добавить колонки category_1, category_2, category_3 с категориями различного уровня"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "path = \"/apps/hive/warehouse/eakotelnikov.db/market_events\"\n",
        "df = spark.read.parquet(path)\n",
        "\n",
        "df.printSchema()\n",
        "df.show(5)\n",
        "\n",
        "split_col = split(df[\"category_code\"], '[.]')\n",
        "df = df.withColumn(\"cat_1\", split_col.getItem(0))\n",
        "df = df.withColumn(\"cat_2\", split_col.getItem(1))\n",
        "df = df.withColumn(\"cat_3\", split_col.getItem(2))\n",
        "\n",
        "df_with_categories = df\n",
        "\n",
        "df_with_categories.show(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Задание 4\n",
        "#### Вывести топ-3 брендов по количеству просмотров для каждой категории 2-го уровня"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import *\n",
        "\n",
        "event_type_filter = col(\"event_type\") == \"view\"\n",
        "\n",
        "df_grouped = df_with_categories. \\\n",
        "                    where(event_type_filter). \\\n",
        "                    groupBy([\"cat_1\", \"cat_2\", \"brand\"]). \\\n",
        "                    agg(count(\"event_type\"). \\\n",
        "                    alias(\"views\")). \\\n",
        "                    orderBy(col(\"views\"). \\\n",
        "                    desc())\n",
        "\n",
        "print(\"Вывод df после группировки\")\n",
        "df_grouped.show(5)\n",
        "\n",
        "\n",
        "Window_Spec  = Window. \\\n",
        "                    partitionBy(\"cat_2\"). \\\n",
        "                    orderBy(col(\"views\"). \\\n",
        "                    desc())\n",
        "                    \n",
        "rank_filter = col(\"rank\") <= 3\n",
        "\n",
        "df_with_ranks = df_grouped. \\\n",
        "                        withColumn(\"rank\",row_number(). \\\n",
        "                        over(Window_Spec)). \\\n",
        "                        orderBy(col(\"views\"). \\\n",
        "                        desc()). \\\n",
        "                        where(rank_filter)\n",
        "\n",
        "\n",
        "print(\"Вывод df после приминения оконной функции\")\n",
        "df_with_ranks.show(5)\n",
        "\n",
        "print(\"Вывод результирующей таблицы\")\n",
        "\n",
        "df_with_ranks. \\\n",
        "        orderBy(col(\"cat_2\").asc(), \\\n",
        "        col(\"views\").desc(), \\\n",
        "        col(\"rank\").asc()). \\\n",
        "        show(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Датасет с треками\n",
        "\n",
        "#### Cоздание hw_3.tracks для запуска локально (на кластере уже готова)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "sch=ArrayType(StringType());\n",
        "\n",
        "# важно что разделитель ', ' с пробелом, иначе пробелы добавятся в значения\n",
        "tracks = spark.read.option(\"header\", \"true\") \\\n",
        "        .option(\"escape\", '\"') \\\n",
        "        .option(\"InferSchema\", \"true\") \\\n",
        "        .csv(\"/datasets/tracks.csv\") \\\n",
        "        .withColumn(\"release_year\", f.substring(\"release_date\", 1, 4).cast(IntegerType())) \\\n",
        "        .withColumn(\"array_artist\", f.split(f.regexp_replace(f.col(\"artists\"), \"[\\]\\[\\']\", \"\"),\", \")) \\\n",
        "        .cache() #выделяем год в отдельную колонку и преобразуем колонку с артистами в массив\n",
        "\n",
        "tracks_exp = tracks.select(  \n",
        "                            \"name\", \n",
        "                            \"popularity\",\n",
        "                            \"danceability\",\n",
        "                            \"energy\",\n",
        "                            \"speechiness\",\n",
        "                            \"acousticness\",\n",
        "                            \"liveness\",\n",
        "                            \"valence\",\n",
        "                            \"release_year\",\n",
        "                            \"artists\",\n",
        "                            f.explode(f.col(\"array_artist\") ).alias(\"name_artist\")\n",
        "                        ) #создаем отдельную таблицу с развернутым массивом артистов\n",
        "                        \n",
        "tracks_exp.printSchema()\n",
        "\n",
        "spark.sql(\"create database hw_3\")\n",
        "tracks_exp.write.mode(\"overwrite\").saveAsTable(\"hw_3.tracks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Задание 5\n",
        "#### Какие артист выпустили наибольшее число песен из годового топ-100 (по популярности)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "tracks = spark.table(\"hw_3.tracks\")\n",
        "z.show(tracks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "from pyspark.sql.window import Window\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "from pyspark.sql.window import Window\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "path = \"/apps/hive/warehouse/hw_3.db/tracks\"\n",
        "df = spark.read.parquet(path)\n",
        "df.printSchema()\n",
        "\n",
        "print(\"--------------------------Initial df--------------------------------------------------------------\")\n",
        "df.show(5)\n",
        "\n",
        "print(\"--------------------------Initial df count--------------------------------------------------------\")\n",
        "print(df.count())\n",
        "df = df.distinct()\n",
        "\n",
        "print(\"--------------------------df count w/o duplicates-------------------------------------------------\")\n",
        "print(df.count())\n",
        "\n",
        "popularity_filter_1 = col(\"popularity\") <= 100\n",
        "popularity_filter_2 = col(\"popularity\") > 0\n",
        "rank_filter = col(\"rank\") <= 100\n",
        "\n",
        "Window_Spec  = Window. \\\n",
        "                    partitionBy(\"release_year\"). \\\n",
        "                    orderBy(col(\"popularity\"). \\\n",
        "                    desc())\n",
        "\n",
        "df_with_ranks = df. \\\n",
        "                where(popularity_filter_1 & popularity_filter_2). \\\n",
        "                withColumn(\"rank\", rank(). \\\n",
        "                over(Window_Spec))\n",
        "                        \n",
        "df_with_ranks = df_with_ranks. \\\n",
        "                filter(rank_filter)\n",
        "\n",
        "                        \n",
        "print(\"--------------------------df with ranks after rank function----------------------------------------\")                        \n",
        "df_with_ranks.show(20)\n",
        "\n",
        "print(\"--------------------------final result with top 100 artist from each year--------------------------\")\n",
        "df_with_ranks. \\\n",
        "    groupBy([\"name_artist\"]). \\\n",
        "    agg(count(\"name\").alias(\"count_artist\")). \\\n",
        "    orderBy(col(\"count_artist\").desc()). \\\n",
        "    show(20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Задание 6\n",
        "#### Вывести топ артистов, которые чаще других попадали в годовой топ-100 песен по популярности?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "   \n",
        "path = \"/apps/hive/warehouse/hw_3.db/tracks\"\n",
        "df = spark.read.parquet(path)\n",
        "df.printSchema()\n",
        "\n",
        "print(\"--------------------------Initial df--------------------------------------------------------------\")\n",
        "df.show(5)\n",
        "\n",
        "print(\"--------------------------Initial df count--------------------------------------------------------\")\n",
        "print(df.count())\n",
        "df = df.distinct()\n",
        "\n",
        "print(\"--------------------------df count w/o duplicates-------------------------------------------------\")\n",
        "print(df.count())\n",
        "\n",
        "popularity_filter_1 = col(\"popularity\") <= 100\n",
        "popularity_filter_2 = col(\"popularity\") > 0\n",
        "rank_filter = col(\"rank\") <= 100\n",
        "\n",
        "Window_Spec  = Window. \\\n",
        "                    partitionBy(\"release_year\"). \\\n",
        "                    orderBy(col(\"popularity\"). \\\n",
        "                    desc())\n",
        "                    \n",
        "df_with_ranks = df. \\\n",
        "                where(popularity_filter_1 & popularity_filter_2). \\\n",
        "                withColumn(\"rank\", rank(). \\\n",
        "                over(Window_Spec))\n",
        "                        \n",
        "df_with_ranks = df_with_ranks. \\\n",
        "                filter(rank_filter)\n",
        "\n",
        "                        \n",
        "print(\"--------------------------df with ranks after rank function----------------------------------------\")                        \n",
        "df_with_ranks.show(5)\n",
        "\n",
        "print(\"--------------------------groupby by year and artist-----------------------------------------------\")\n",
        "df_result = df_with_ranks. \\\n",
        "    groupBy([\"release_year\", \"name_artist\"]). \\\n",
        "    agg(count(\"name\").alias(\"count_songs\")). \\\n",
        "    orderBy(col(\"count_songs\").desc()) \\\n",
        "    \n",
        "df_result.show(20)\n",
        "\n",
        "print(\"--------------------------final result with the most frequent artists in top 100--------------------\")\n",
        "df_result = df_result. \\\n",
        "    groupBy([\"name_artist\"]). \\\n",
        "    agg(count(\"release_year\").alias(\"count_artist\")). \\\n",
        "    orderBy(col(\"count_artist\").desc()). \\\n",
        "    show(20)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Задание 7.1\n",
        "#### Какие артисты дольше других несколько лет подряд держались в ежегодном топ-100 песен по популярности?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "#import pyspark.sql.functions as f\n",
        "   \n",
        "path = \"/apps/hive/warehouse/hw_3.db/tracks\"\n",
        "df = spark.read.parquet(path)\n",
        "df.printSchema()\n",
        "\n",
        "df.show(5)\n",
        "df = df.distinct()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Window_Spec = Window. \\\n",
        "              partitionBy(\"release_year\"). \\\n",
        "              orderBy(col(\"popularity\").desc())\n",
        "\n",
        "\n",
        "df_result = df.withColumn(\"rank\", row_number().over(Window_Spec)). \\\n",
        "                    filter(col(\"rank\") <= 100). \\\n",
        "                    drop(col(\"rank\")). \\\n",
        "                    select('release_year', 'name_artist'). \\\n",
        "                    distinct(). \\\n",
        "                    sort(\"name_artist\", \"release_year\", ascending=[True, True])\n",
        "\n",
        "\n",
        "Window_Spec  = Window. \\\n",
        "               partitionBy(\"name_artist\"). \\\n",
        "               orderBy(col(\"release_year\").asc())\n",
        "                \n",
        "                \n",
        "df_result_with_lag = df_result. \\\n",
        "                     withColumn(\"lag\", lag(\"release_year\", 1).over(Window_Spec))\n",
        "\n",
        "\n",
        "yearDifference = col(\"release_year\") - col(\"lag\")\n",
        "df_result_with_lag = df_result_with_lag. \\\n",
        "                     withColumn(\"yearDifference\", yearDifference)\n",
        "\n",
        "\n",
        "df_result_with_counter = df_result_with_lag. \\\n",
        "                         withColumn(\"counter\", when(col(\"yearDifference\") == 1, 1).when(col(\"yearDifference\") != 1, 0).otherwise(0))\n",
        "\n",
        "\n",
        "final_result = df_result_with_counter. \\\n",
        "               groupBy([\"name_artist\"]). \\\n",
        "               agg(sum(\"counter\").alias(\"count\")). \\\n",
        "               orderBy(col(\"count\").desc())\n",
        "               \n",
        "#df_result_with_counter = df_result_with_counter.withColumn(\"id\", monotonically_increasing_id())      \n",
        "               \n",
        "#new_result = df_result_with_counter.withColumn(\n",
        "#    \"grp\", \n",
        "#    sum((col(\"counter\") == 0).cast(\"int\")).over(Window.orderBy(\"id\"))\n",
        "#    ).withColumn(\n",
        "#    \"D\",\n",
        "#    sum(col(\"counter\")).over(Window.partitionBy(\"grp\").orderBy(\"id\"))\n",
        "#).drop(\"grp\")\n",
        "    \n",
        "#new_result = new_result.groupBy([\"name_artist\"]).agg(count(\"D\").alias(\"count\")).orderBy(col(\"count\").desc())\n",
        "#z.show(new_result)\n",
        "\n",
        "final_result.show(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Задание 7.2\n",
        "#### Решение с udf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "path = \"/apps/hive/warehouse/hw_3.db/tracks\"\n",
        "df = spark.read.parquet(path)\n",
        "df.printSchema()\n",
        "\n",
        "df = df.distinct()\n",
        "\n",
        "Window_Spec = Window.partitionBy(\"release_year\").orderBy(col(\"popularity\").desc())\n",
        "\n",
        "\n",
        "df = df.withColumn(\"rank\", row_number().over(Window_Spec)). \\\n",
        "                    filter(col(\"rank\") <= 100). \\\n",
        "                    drop(col(\"rank\"))\n",
        "\n",
        "# Подсчет количества лет подряд для каждого артиста\n",
        "@udf(returnType=IntegerType())\n",
        "def count_consecutive_years(release_years):\n",
        "    consecutive_years = 0\n",
        "    prev_year = None\n",
        "    for year in sorted(release_years):\n",
        "        if prev_year is not None and year == prev_year + 1:\n",
        "            consecutive_years += 1\n",
        "        prev_year = year\n",
        "    return consecutive_years\n",
        "\n",
        "\n",
        "result_top_artists = df.groupBy(\"name_artist\"). \\\n",
        "                                      agg(collect_list(\"release_year\").alias(\"release_years\")). \\\n",
        "                                      withColumn(\"max_period\", count_consecutive_years(col(\"release_years\"))). \\\n",
        "                                      filter(col(\"max_period\") >= 2). \\\n",
        "                                      select(col(\"name_artist\"), col(\"max_period\")). \\\n",
        "                                      orderBy(col(\"max_period\").desc())\n",
        "\n",
        "result_top_artists.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Задание 7.3\n",
        "#### Через lag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "\n",
        "# https://stackoverflow.com/questions/56384625/pyspark-cumulative-sum-with-reset-condition\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "import pyspark.sql.functions as f\n",
        "   \n",
        "path = \"/apps/hive/warehouse/hw_3.db/tracks\"\n",
        "df = spark.read.parquet(path)\n",
        "df.printSchema()\n",
        "\n",
        "print(\"--------------------------OPTION 1--------------------------\")\n",
        "\n",
        "print(\"--------------------------Initial df--------------------------\")\n",
        "df.show(5)\n",
        "print(\"--------------------------Initial df count--------------------------\")\n",
        "print(df.count())\n",
        "df = df.distinct()\n",
        "print(\"--------------------------df count w/o duplicates--------------------------\")\n",
        "print(df.count())\n",
        "\n",
        "popularity_filter_1 = col(\"popularity\") <= 100\n",
        "popularity_filter_2 = col(\"popularity\") > 0\n",
        "\n",
        "\n",
        "df_result = df.where(popularity_filter_1 & popularity_filter_2).select('release_year', 'name_artist').distinct().sort(\"name_artist\", \"release_year\", ascending=[True, True])\n",
        "z.show(df_result)\n",
        "\n",
        "Window_Spec  = Window.partitionBy(\"name_artist\").orderBy(col(\"release_year\").asc())\n",
        "df_result_with_lag = df_result.withColumn(\"lag\", lag(\"release_year\", 1).over(Window_Spec))\n",
        "z.show(df_result_with_lag)\n",
        "\n",
        "yearDifference = col(\"release_year\") - col(\"lag\")\n",
        "df_result_with_lag = df_result_with_lag.withColumn(\"yearDifference\", yearDifference)\n",
        "z.show(df_result_with_lag)\n",
        "\n",
        "df_result_with_counter = df_result_with_lag.withColumn(\"counter\", when(col(\"yearDifference\") == 1, 1).when(col(\"yearDifference\") != 1, 0).otherwise(0))\n",
        "z.show(df_result_with_counter)\n",
        "\n",
        "df_result_with_counter = df_result_with_counter.withColumn(\"id\", monotonically_increasing_id())\n",
        "z.show(df_result_with_counter)\n",
        "\n",
        "new_result = df_result_with_counter.withColumn(\n",
        "    \"grp\", \n",
        "    f.sum((f.col(\"counter\") == 0).cast(\"int\")).over(Window.orderBy(\"id\"))\n",
        "    ).withColumn(\n",
        "    \"D\",\n",
        "    f.sum(f.col(\"counter\")).over(Window.partitionBy(\"grp\").orderBy(\"id\"))\n",
        ").drop(\"grp\")\n",
        "    \n",
        "new_result = new_result.groupBy([\"name_artist\"]).agg(count(\"D\").alias(\"count\")).orderBy(col(\"count\").desc())\n",
        "z.show(new_result)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#final_result = df_result_with_counter.groupBy([\"name_artist\"]).agg(count(\"counter\").alias(\"count\")).orderBy(col(\"count\").desc())\n",
        "#z.show(final_result)\n",
        "\n",
        "\n",
        "\n",
        "#final_result = df_result_with_counter.groupBy([\"name_artist\"]).agg(count(\"counter\").alias(\"count\")).orderBy(col(\"count\").desc())\n",
        "#z.show(final_result)\n",
        "\n",
        "#df_result.show(10)\n",
        "#yearDifference = col(\"release_year\") - col(\"lag\")\n",
        "#df_result_final = df_result.withColumn(\"yearDifference\", yearDifference). \\\n",
        "#                withColumn(\"Trend\", when(col(\"yearDifference\") == 0, 0).when(col(\"yearDifference\") > 0, 1).otherwise(0)).orderBy(col(\"name_artist\").desc(), col(\"release_year\").asc())\n",
        "                \n",
        "#df_result_final.show(20)         \n",
        "                \n",
        "#.filter(df_result_final.Trend.isNotNull())\n",
        "\n",
        "#df_result_final.withColumn(\n",
        "#    \"grp\", \n",
        "#    f.sum((f.col(\"Trend\") == 0).cast(\"int\")).over(Window.orderBy(\"release_year\"))\n",
        "#).withColumn(\n",
        "#    \"D\",\n",
        "#    f.sum(f.col(\"Trend\")).over(Window.partitionBy(\"grp\").orderBy(\"release_year\"))\n",
        "#).drop(\"grp\").show()\n",
        "\n",
        "#.withColumn(\n",
        "#    \"result\",\n",
        "#    f.sum(f.col(\"Trend\")).over(Window.partitionBy(\"grp\").orderBy(\"release_year\"))\n",
        "#).drop(\"grp\").show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "name": "homework_5"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
