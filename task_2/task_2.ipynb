{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "\n",
        "%spark.pyspark\n",
        "\n",
        "path = \"/apps/hive/warehouse/eakotelnikov.db/market_events\" # Здесь указываем путь до папки с паркетами (но не глубже). Проверяем столбец event_date появился. \n",
        "\n",
        "df = spark.read.parquet(path)\n",
        "\n",
        "df.printSchema()\n",
        "\n",
        "z.show(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Вывести топ категорий по количеству просмотров товаров за всё время"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "# вывести в виде\n",
        "# +--------------------------------+----------+\n",
        "# |category_code                   |view_count|\n",
        "# +--------------------------------+----------+\n",
        "# |null                            |20837460  |\n",
        "# |electronics.smartphone          |14832387  |\n",
        "# |computers.notebook              |2103024   |\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "view_filter = col(\"event_type\") == \"view\"\n",
        "\n",
        "(\n",
        "    df\n",
        "    .where(view_filter)\n",
        "    .groupBy(\"category_code\")\n",
        "    .agg(count(\"event_type\").alias(\"view_count\"))\n",
        "    .orderBy(col(\"view_count\")\n",
        "    .desc())\n",
        "    .show(truncate=False) \n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Построить гистограмму распределения цен на проданные товары за 10 октября"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "# результат визуализировать через z.show()\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "#from pyspark.sql.functions import year, month, dayofmonth, max, min\n",
        "#from pyspark.sql.functions import col\n",
        "\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.feature import Bucketizer\n",
        "\n",
        "day_filter = col(\"event_date\") == \"2019-11-10\"\n",
        "event_type_filter = col(\"event_type\") == \"purchase\"\n",
        "\n",
        "(\n",
        "    df\n",
        "    .where(day_filter & event_type_filter)\n",
        "    .select(max(df.price))\n",
        "    .show()\n",
        ")\n",
        "\n",
        "bucketizer = Bucketizer(splits=[i*10 for i in range(259)], inputCol=\"price\", outputCol=\"bin\")\n",
        "\n",
        "df_buck = (\n",
        "            bucketizer\n",
        "                    .setHandleInvalid(\"keep\")\n",
        "                    .transform(df.where(day_filter & event_type_filter))\n",
        "                    .select(col(\"price\"),col(\"bin\"))\n",
        "                    .groupBy(\"bin\")\n",
        "                    .agg(count(\"price\")\n",
        "                    .alias(\"price_count\"))\n",
        "                    .orderBy(col(\"bin\")\n",
        "                    .asc())\n",
        ")                    \n",
        "                    \n",
        "df_buck.select(min(df_buck.bin)).show()\n",
        "df_buck.select(max(df_buck.bin)).show()\n",
        "z.show(df_buck)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Посчитать количество продаж за октябрь отдельно бренда apple и остальных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "# вывести через df.show() в виде\n",
        "# +--------+------+\n",
        "# |is_apple| count|\n",
        "# +--------+------+\n",
        "# |    true| *****|\n",
        "# |   false| *****|\n",
        "# +--------+------+\n",
        "\n",
        "from pyspark.sql import functions as f\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "df.printSchema()\n",
        "\n",
        "view_filter = col(\"event_type\") == \"purchase\"\n",
        "\n",
        "(\n",
        "    df\n",
        "    .where(view_filter)\n",
        "    .withColumn('is_apple', f.when(f.col('brand') == \"apple\", \"true\")\n",
        "    .otherwise(\"false\"))\n",
        "    .groupBy(\"is_apple\")\n",
        "    .agg(count(\"event_type\")\n",
        "    .alias(\"count\"))\n",
        "    .orderBy(col(\"is_apple\")\n",
        "    .desc())\n",
        "    .show()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Построить почасовой график продаж и прибыли за вторую неделю октября"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# z.show(), ключ -- часы, значения -- число продаж и сумма прибыли за этот час\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "print(df.agg({\"event_date\": \"min\"}).collect()[0])\n",
        "print(df.agg({\"event_date\": \"max\"}).collect()[0])\n",
        "\n",
        "df_week = df.withColumn(\"week_number\", date_format(to_date(\"event_date\", \"yyyy-MM-dd\"), \"W\"))\n",
        "\n",
        "print(df_week.agg({\"week_number\": \"min\"}).collect()[0])\n",
        "print(df_week.agg({\"week_number\": \"max\"}).collect()[0])\n",
        "\n",
        "df_week_hour = (\n",
        "                df\n",
        "                .withColumn(\"week_number\", date_format(to_date(\"event_date\", \"yyyy-MM-dd\"), \"W\"))\n",
        "                .withColumn(\"hour\", hour(col(\"event_time\")))\n",
        "                .cache()\n",
        "                )\n",
        "\n",
        "week_number_filter = col(\"week_number\") == 2\n",
        "event_type_filter = col(\"event_type\") == \"purchase\"\n",
        "\n",
        "df_filtered_by_hour_week = df_week_hour.where(week_number_filter & event_type_filter).cache()\n",
        "\n",
        "result = (\n",
        "        df_filtered_by_hour_week\n",
        "        .groupBy(\"hour\")\n",
        "        .agg(count(\"*\")\n",
        "        .alias(\"amount_of_sales\"), sum(\"price\").alias(\"profit\"))\n",
        "        .orderBy(col(\"hour\").asc())\n",
        "        )\n",
        "\n",
        "z.show(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "z.show(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Построить почасовой график числа продаж и выручки, усредненный за месяц"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# z.show(), ключ -- час в диапазоне от 0 до 23, значение -- усредненное за месяц число продаж в этот час на месячных данных\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "df.printSchema()\n",
        "\n",
        "view_filter = col(\"event_type\") == \"purchase\"\n",
        "df_intermidiate = (\n",
        "                    df.where(view_filter)\n",
        "                    .withColumn(\"hour\", hour(df.event_time))\n",
        "                    .withColumn(\"month\", month(df.event_time))\n",
        "                    )\n",
        "\n",
        "windowSpec = Window.partitionBy(\"month\").orderBy(\"hour\")\n",
        "result = (\n",
        "            df_intermidiate\n",
        "            .withColumn(\"avg_price\", avg(col(\"price\")).over(windowSpec))\n",
        "            .withColumn(\"count_prices\", count(col(\"price\")).over(windowSpec))\n",
        "            .groupBy(\"hour\")\n",
        "            .agg(sum(\"avg_price\").alias(\"avg_revenues_based_on_month\"), count(\"*\").alias(\"avg_amount_of_sales_based_on_month\"))\n",
        "            .orderBy(\"hour\")\n",
        "            )\n",
        "            \n",
        "z.show(result)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "z.show(result)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "name": "homework_3"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
