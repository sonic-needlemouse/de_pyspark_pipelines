{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сохранить один месяц данных из папки /datasets/marketplace в личную базу username.market_events, данные партиционировать по дате.\n",
    "#### Посчитать посуточные аггрегаты для этих данных по категориям в таблицу event_types_daily с колонками: event_date, category_id, event_type, event_count, distinct_customer_count. \n",
    "#### Счёт нужно производить отдельно по дням (в цикле по датам), целевую таблицу тоже разделить на партици.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "\n",
    "from pyspark.sql.functions import col, to_timestamp, to_date\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "spark.sql(\"create database if not exists sergeev\") # Создание БД\n",
    "\n",
    "#market_df = spark.read.csv(\"/datasets/marketplace/2019-Nov.csv.gz\", header = True, inferSchema=True) # Считывание файла из csv формата\n",
    "\n",
    "#market_df.show(truncate = False) # Вывод таблицы\n",
    "#market_df.printSchema() # Вывод схемы данных\n",
    "\n",
    "# Столбец event_time в формате string. Перед партиционированием необходимо перевести timestamp\n",
    "\n",
    "#(\n",
    "#market_df\n",
    "#.withColumn(\"event_time\", to_timestamp(col(\"event_time\"), 'yyyy-MM-dd HH:mm:ss'))\n",
    "#.withColumn(\"event_date\", to_date(col(\"event_time\")))\n",
    "#.write\n",
    "#.partitionBy(\"event_date\")\n",
    "#.mode(\"overwrite\")\n",
    "#.saveAsTable(\"sergeev.market_events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "\n",
    "df = spark.table(\"sergeev.market_events\")\n",
    "df.printSchema()\n",
    "\n",
    "z.show(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "\n",
    "parquet_daily_file = \"/apps/hive/warehouse/sergeev.db/market_events\" # Здесь указываем путь до папки с паркетами (но не глубже). Проверяем столбец event_date появился. \n",
    "\n",
    "df = spark.read.parquet(parquet_daily_file)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "#z.show(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "\n",
    "\n",
    "df.createOrReplaceTempView(\"temp_df\") # Создаем временный датафрейм. В дальнейшем будем с ним работать\n",
    "\n",
    "spark.sql(\"describe table temp_df\").show(truncate=False) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "\n",
    "unique_date_values = df.select(\"event_date\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "\n",
    "for date in unique_date_values:\n",
    "    print(date)\n",
    "    query = \"select event_date, category_id, event_type, count(event_time) as event_count, count(distinct user_id) as distinct_customer_count from temp_df where event_date ='{}' group by event_date, category_id, event_type\".format(date)\n",
    "    #spark.sql(query).show()\n",
    "    spark.sql(query).write.partitionBy(\"event_date\").mode(\"append\").saveAsTable(\"sergeev.event_types_daily\")\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "name": "homework_2"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
