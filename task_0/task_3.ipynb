{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.conf\n",
    "\n",
    "spark.executor.instances=2\n",
    "spark.executor.memory=1G\n",
    "spark.kryoserializer.buffer.max=1024m\n",
    "\n",
    "spark.sql.autoBroadcastJoinThreshold=20971520"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно скопировать себе эту тетрадку. Параграфы с генерацией данных и созданием семплов запускать не нужно, они оставлены для ознакомления"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.random.RandomRDDs._\n",
    "import java.time.LocalDate\n",
    "import java.time.format.DateTimeFormatter\n",
    "\n",
    "val dates = (0 to 14).map(LocalDate.of(2020, 11, 1).plusDays(_).format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd\"))).toSeq\n",
    "\n",
    "def generateCity(r: Double): String = if (r < 0.9) \"BIG_CITY\" else \"SMALL_CITY_\" + scala.math.round((r - 0.9) * 1000)\n",
    "\n",
    "def generateCityUdf = udf(generateCity _)\n",
    "\n",
    "// spark.sql(\"drop table hw2.events_full\")\n",
    "spark.sql(\"create database hw_4\")\n",
    "for(i <- dates) {\n",
    "    uniformRDD(sc, 10000000L, 1)\n",
    "    .toDF(\"uid\")\n",
    "    .withColumn(\"date\", lit(i))\n",
    "    .withColumn(\"city\", generateCityUdf($\"uid\"))\n",
    "    .selectExpr(\"date\", \" sha2(cast(uid as STRING), 256) event_id\", \"city\")\n",
    "    .withColumn(\"skew_key\", when($\"city\" === \"BIG_CITY\", lit(\"big_event\")).otherwise($\"event_id\"))\n",
    "    .write.mode(\"append\")\n",
    "    .partitionBy(\"date\")\n",
    "    .saveAsTable(\"hw_4.events_full\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.table(\"hw_4.events_full\")\n",
    ".select(\"event_id\")\n",
    ".sample(0.001)\n",
    ".repartition(2)\n",
    ".write.mode(\"overwrite\")\n",
    ".saveAsTable(\"hw_4.sample\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.table(\"hw_4.sample\")\n",
    ".limit(100)\n",
    ".coalesce(1)\n",
    ".write.mode(\"overwrite\")\n",
    ".saveAsTable(\"hw_4.sample_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "spark.table(\"hw_4.events_full\")\n",
    ".select(\"event_id\")\n",
    ".sample(0.003)\n",
    ".repartition(1)\n",
    ".write.mode(\"overwrite\")\n",
    ".saveAsTable(\"hw_4.sample_big\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "spark.table(\"hw_4.events_full\")\n",
    ".select(\"event_id\")\n",
    ".sample(0.015)\n",
    ".repartition(1)\n",
    ".write.mode(\"overwrite\")\n",
    ".saveAsTable(\"hw_4.sample_very_big\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 1\n",
    "Для упражнений сгрененирован большой набор синтетических данных в таблице hw4.events_full. Из этого набора данных созданы маленькие (относительно исходного набора) таблицы разного размера kotelnikov.sample_[small, big, very_big]. \n",
    "\n",
    "Ответить на вопросы:\n",
    " * какова структура таблиц\n",
    " * сколько в них записей \n",
    " * сколько места занимают данные\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "tables = spark.sql(\"SHOW TABLES IN hw_4\").collect()\n",
    "    \n",
    "for row in tables:\n",
    "    print(row.tableName)\n",
    "    spark.table(\"{}.{}\".format(row.database, row.tableName)).printSchema()\n",
    "    spark.table(\"{}.{}\".format(row.database, row.tableName)).count()\n",
    "    print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 2\n",
    "Получить планы запросов для джойна большой таблицы hw_4.events_full с каждой из таблиц hw_4.sample, hw_4.sample_big, hw_4.sample_very_big по полю event_id. В каких случаях используется BroadcastHashJoin? \n",
    "\n",
    "BroadcastHashJoin автоматически выполняется для джойна с таблицами, размером меньше параметра spark.sql.autoBroadcastJoinThreshold. Узнать его значение можно командой spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\").\n",
    "\n",
    "Решение: ячейка ниже.\n",
    "BroadcastHashJoin используется при джоине events_full и sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "print(\"autoBroadcastJoinThreshold =\", spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\"))\n",
    "\n",
    "events_full = spark.table(\"hw_4.events_full\")\n",
    "sample = spark.table(\"hw_4.sample\")\n",
    "sample_big = spark.table(\"hw_4.sample_big\")\n",
    "sample_very_big = spark.table(\"hw_4.sample_very_big\")\n",
    "\n",
    "#events_full.show()\n",
    "print(\"size of the events_full table:\", events_full.count())\n",
    "print(\"size of the sample table:\", sample.count())\n",
    "print(\"size of the sample_big table:\", sample_big.count())\n",
    "print(\"size of the sample_very_big table:\", sample_very_big.count())\n",
    "\n",
    "events_full.join(sample, \"event_id\", \"inner\").explain()\n",
    "print('-------------------------------------------------------')\n",
    "events_full.join(sample_big, \"event_id\", \"inner\").explain()\n",
    "print('-------------------------------------------------------')\n",
    "events_full.join(sample_very_big, \"event_id\", \"inner\").explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3\n",
    "\n",
    "Выполнить джойны с таблицами  hw_4.sample,  hw_4.sample_big в отдельных параграфах, чтобы узнать время выполнения запросов (например, вызвать .count() для результатов запросов). Время выполнения параграфа считается автоматически и указывается в нижней части по завершении.\n",
    "\n",
    "events_full join sample время выполнения состовляет 20 секунд. Размер таблицы на выходе: 150397\n",
    "events_full join sample_big время выполнения составляет 1 минута 3 секунды. Размер таблицы на выходе: 449135\n",
    "\n",
    "Зайти в spark ui (ссылку сгенерировать в следующем папраграфе). Сколько tasks создано на каждую операцию? Почему именно столько? Каков DAG вычислений?  \n",
    "\n",
    "events_full join sample: 82 tasks. Всего 2 стейджа: 91 и 92. \n",
    "Стейдж 91: FileScanRDD -> MapPartitionsRDD. Происходит поиск партиций и мэппинг. \n",
    "Стейдж 92: ShuffledRowRDD -> MapPartitionsRDD -> MapPartitionsRDD. Происходит shuffle данных по индексам значений, которые мы мерджим и затем партиционирование и мэпинг.\n",
    "По первой таблице происходит Scan parquet -> Filter -> Project. По второй таблице (sample) идет Scan parquet -> Filter -> Project -> BroadcastExchange -> Происходит BroadcastHashJoin c первой таблицей\n",
    "http://ca-spark-n-01.innoca.local:8088/proxy/application_1707128095337_0216/jobs/job?id=58 \n",
    "http://ca-spark-n-01.innoca.local:8088/proxy/application_1707128095337_0254/SQL/execution?id=4\n",
    "\n",
    "events_full join sample_big: 284 tasks. Всего 4 стейджа: 8, 9, 10 и 11. \n",
    "Стейдж 8: FileScanRDD -> MapPartitionsRDD. Происходит поиск партиций и мэппинг. \n",
    "Стейдж 9: FileScanRDD -> MapPartitionsRDD. Происходит поиск партиций и мэппинг.\n",
    "Стейдж 10: ShuffledRowRDD -> MapPartitionsRDD -> ZippedPartitionsRDD -> MapPartitionsRDD. На данном этапе происходит дополнительный shuffle.\n",
    "Стейдж 11: ShuffledRowRDD -> MapPartitionsRDD -> MapPartitionsRDD. Происходит shuffle данных по индексам значений, которые мы мерджим и затем партиционирование и мэпинг.\n",
    "Главная разница по сравнению с предыдущим запросом (events_full join sample), что джоин происходит через SortMergeJoin, а не через BroadcastHashJoin, так как вторая таблица (sample_big) весит 27mb, что больше параметра spark.sql.autoBroadcastJoinThreshold равного 20mb \n",
    "http://ca-spark-n-01.innoca.local:8088/proxy/application_1707128095337_0219/jobs/job?id=4\n",
    "http://ca-spark-n-01.innoca.local:8088/proxy/application_1707128095337_0254/SQL/execution?id=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "sc.setLocalProperty(\"callSite.short\", \"events_full join sample\")\n",
    "\n",
    "print(events_full.join(sample, \"event_id\", \"inner\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "\n",
    "sc.setLocalProperty(\"callSite.short\", \"events_full join sample_big\")\n",
    "\n",
    "print(events_full.join(sample_big, \"event_id\", \"inner\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 4.1\n",
    "#### Насильный broadcast\n",
    "\n",
    "Оптимизировать джойн с таблицами hw_4.sample_big, hw_4.sample_very_big с помощью broadcast(df). Выполнить запрос, посмотреть в UI, как поменялся план запроса, DAG, количество тасков. Второй запрос не выполнится.\n",
    "\n",
    "events_full join broadcast sample_big время выполнения состовляет 33 секунды. \n",
    "82tasks. Всего 2 стейджа: 21, 22.\n",
    "Стейдж 21: FileScanRDD -> MapPartitionsRDD. Происходит поиск партиций и мэппинг.\n",
    "Стейдж 22: ShuffledRowRDD -> MapPartitionsRDD -> MapPartitionsRDD -> MapPartitionsRDD -> FileScanRDD -> MapPartitionsRDD. Происходит shuffle данных по индексам значений, которые мы мерджим и затем партиционирование и мэпинг.\n",
    "Джоин происходит через BroadcastHashJoin\n",
    "http://ca-spark-n-01.innoca.local:8088/proxy/application_1707128095337_0254/SQL/execution?id=6\n",
    "\n",
    "events_full join broadcast sample_very_big запрос не выполнился, так как размер sample_very_big, помещенный в broadcast состовляет 136mb, что больше значения spark.sql.autoBroadcastJoinThreshold равного 20mb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "sc.setLocalProperty(\"callSite.short\", \"events_full join broadcast sample_big\")\n",
    "\n",
    "print(events_full.join(broadcast(sample_big), \"event_id\", \"inner\").count())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "sc.setLocalProperty(\"callSite.short\", \"events_full join broadcast sample_very_big\")\n",
    "\n",
    "print(events_full.join(broadcast(sample_very_big), \"event_id\", \"inner\").count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таблица hw_4.sample_very_big оказывается слишком большой для бродкаста и не помещается полностью на каждой ноде, поэтому возникает исключение.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 4.2\n",
    "#### Отключение auto broadcast\n",
    "\n",
    "Отключить автоматический броадкаст командой spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\"). Сделать джойн с семплом hw_4.sample, сравнить время выполнения запроса.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "sc.setLocalProperty(\"callSite.short\", \"events_full join sample\")\n",
    "\n",
    "print(events_full.join(sample, \"event_id\", \"inner\").count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 минут 2 секунды в случае, когда отключен автоматический броадкастинг, по сравнению с 2 минутами 1 секундой со включенным бродкастингом.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"26214400\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.sql(\"clear cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 5.1\n",
    "\n",
    "В процессе обработки данных может возникнуть перекос объёма партиций по количеству данных (data skew). В таком случае время выполнения запроса может существенно увеличиться, так как данные распределятся по исполнителям неравномерно. В следующем параграфе происходит инициализация датафрейма, этот параграф нужно выполнить, изменять код нельзя. В задании нужно работать с инициализированным датафреймом.\n",
    "\n",
    "Датафрейм разделен на 30 партиций по ключу city, который имеет сильно  неравномерное распределение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark \n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "skew_df = spark.table(\"hw_4.events_full\")\\\n",
    ".where(\"date = '2020-11-01'\")\\\n",
    ".repartition(30, col(\"city\"))\\\n",
    ".cache()\n",
    "\n",
    "skew_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 5.2\n",
    "\n",
    "Посчитать количество event_count различных событий event_id , содержащихся в skew_df с группировкой по городам. Результат упорядочить по event_count.\n",
    "\n",
    "В spark ui в разделе jobs выбрать последнюю, в ней зайти в stage, состоящую из 30 тасков (из такого количества партиций состоит skew_df). На странице стейджа нажать кнопку Event Timeline и увидеть время выполнения тасков по экзекьюторам. Одному из них выпала партиция с существенно большим количеством данных. Остальные экзекьюторы в это время бездействуют -- это и является проблемой, которую предлагается решить далее.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.functions import *\n",
    "sc.setLocalProperty(\"callSite.short\", \"skew task\")\n",
    "\n",
    "skew_df.printSchema()\n",
    "skew_df.show()\n",
    "\n",
    "skew_df \\\n",
    ".groupBy(\"city\") \\\n",
    ".agg(count(\"event_id\").alias(\"event_count\")) \\\n",
    ".orderBy(col(\"event_count\").desc()) \\\n",
    ".show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 5.3\n",
    "\n",
    "Один из способов решения проблемы агрегации по неравномерно распределенному ключу является предварительное перемешивание данных. Его можно сделать с помощью метода repartition(p_num), где p_num -- количество партиций, на которые будет перемешан исходный датафрейм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "sc.setLocalProperty(\"callSite.short\", \"repartition_task\")\n",
    "skew_df = spark.table(\"hw_4.events_full\")\n",
    "skew_df.repartition(30).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 5.4\n",
    "\n",
    "Другой способ исправить неравномерность по ключу -- создание синтетического ключа с равномерным распределением. В нашем случае неравномерность исходит от единственного значения city='BIG_CITY', которое часто повторяется в данных и при группировке попадает к одному экзекьютору. В таком случае лучше провести группировку в два этапа по синтетическому ключу CITY_SALT, который принимает значение BIG_CITY_rand (rand -- случайное целое число) для популярного значения BIG_CITY и CITY для остальных значений. На втором этапе восстанавливаем значения CITY и проводим повторную агрегацию, которая не занимает времени, потому что проводится по существенно меньшего размера данным. \n",
    "\n",
    "Такая же техника применима и к джойнам по неравномерному ключу, см, например https://itnext.io/handling-data-skew-in-apache-spark-9f56343e58e8\n",
    "\n",
    "Что нужно реализовать:\n",
    "* добавить синтетический ключ\n",
    "* группировка по синтетическому ключу\n",
    "* восстановление исходного значения\n",
    "* группировка по исходной колонке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Добавляем колонку с \"солью\": для BIG_CITY - случайное целое от 0 до 20 включительно, для SMALL_CITY - 21, \n",
    "# и выводим кусочки датафрейма для BIG_CITY и SMALL_CITY для контроля правильности выполненной процедуры\n",
    "\n",
    "sc.setLocalProperty(\"callSite.short\", \"big_city_select_and_salt\")\n",
    "\n",
    "\n",
    "events_full = spark.table(\"hw_4.events_full\")\n",
    "\n",
    "print(\"Количество записей в таблице:\", events_full.count())\n",
    "print(\"-------------------------------------------------------------------------\")\n",
    "\n",
    "salt = f.expr(\"\"\"pmod(round(rand() * 1000000, 0), 23)\"\"\").cast(\"integer\")\n",
    "salted = events_full.withColumn(\"salt\", salt)\n",
    "\n",
    "print(\"Уникальные значения синтетического ключа salt\")\n",
    "\n",
    "salted \\\n",
    "    .select('salt') \\\n",
    "    .distinct() \\\n",
    "    .orderBy(f.col(\"salt\").asc()) \\\n",
    "    .show(30) \\\n",
    "    \n",
    "print(\"-------------------------------------------------------------------------\")\n",
    "print(\"Выыод рандомных семплов\")\n",
    "    \n",
    "salted.sample(0.1).show(30, False)\n",
    "\n",
    "print(\"-------------------------------------------------------------------------\")\n",
    "\n",
    "salted_agg_df = salted \\\n",
    ".groupBy(\"salt\", \"city\")\\\n",
    ".agg(f.count(\"*\").alias(\"count\")) \\\n",
    "    .orderBy(f.col(\"count\").desc())\n",
    "    \n",
    "print(\"Вывод группировки по синтетическому ключу salt\")\n",
    "    \n",
    "salted_agg_df.show(30, False)\n",
    "\n",
    "print(\"-------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"Вывод группировки по исходному ключу\")\n",
    "\n",
    "agg_df = salted_agg_df \\\n",
    "    .groupBy(f.col(\"city\")).agg(f.sum(\"count\").alias(\"final_sum\")) \\\n",
    "    .orderBy(f.col(\"final_sum\").desc())\n",
    "    \n",
    "agg_df.show(30, False)\n",
    "\n",
    "print(\"-------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"Вывод количества событий в таблице-результате\")\n",
    "\n",
    "agg_df.agg(f.sum(\"final_sum\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "name": "homework_4"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
