# Автоматизированный Data Pipeline с использованием Apache Airflow и Spark

## Общее описание

Этот репозиторий содержит код DAG для Apache Airflow и соответствующий Scala-скрипт для Apache Spark. DAG автоматизирует процесс проверки наличия таблицы на кластере Hadoop и, в случае её существования, подсчёт количества строк в ней. Результаты работы DAG отправляются по электронной почте.

## Описание задач

DAG выполняет следующие задачи каждые 4 часа:

1. Проверка наличия заданной таблицы на кластере Hadoop.
2. Отправка уведомления по электронной почте, если таблица отсутствует.
3. Подсчет количества строк в таблице, если она существует.
4. Отправка результата подсчета количества строк по электронной почте.

## Компоненты

- **DAG `student52_hw6`**:
  - Использует `HiveOperator` для проверки наличия таблицы.
  - `EmailOperator` для уведомления о результатах проверки.
  - `SparkSubmitOperator` для подсчета строк в таблице с помощью Spark.
  - `PythonOperator` для чтения результата подсчета строк и передачи его следующей задаче.

- **Scala-скрипт `CountTableRows`**:
  - Подключается к Hive.
  - Выполняет SQL запрос для подсчета строк в таблице.
  - Сохраняет результат в HDFS.

## Настройка и запуск

1. Установите Apache Airflow и Spark на вашем кластере.
2. Разместите код DAG в директории `/opt/airflow/dags` на сервере Airflow.
3. Укажите имя таблицы и другие настройки через Airflow Variables.
4. Установите соответствующие соединения в Airflow для Hive и Spark.
5. Запустите DAG через Airflow Web Interface или CLI.
