# Работа с данными в Hadoop и Postgresql

## Описание проекта

Данный проект включает в себя серию задач для работы с данными в Hadoop и Postgresql, используя Apache Spark для выполнения ETL-операций. Задачи включают загрузку данных из таблиц Postgresql в Hadoop, создание широкой таблицы из этих данных, а также создание и сохранение отфильтрованных данных по определенным критериям.

## Исходная задача

1. Установить sftp клиент, подключиться к 10.4.107.88:22. 
2. В папке dags создать даг с 1 таской, выводящей в логи контекст исполнения PythonOperator
3. Сделать даг с расписанием @once, который создаст папку /opt/airflow2/data/{username}
4. Сделать даг с генерацией данных в папку /opt/airflow2/data/{username}/calls по примеру дага с лекции. Данные должны быть видны в папке data на sftp сервере.

## Инструкции по запуску

Во всех дагах указать лекцию (lesson_1) и свой username в тегах.
